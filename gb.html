

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Gradient boosting &mdash; hep_ml 0.2 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="hep_ml 0.2 documentation" href="index.html"/>
        <link rel="next" title="uBoost" href="uboost.html"/>
        <link rel="prev" title="hep_ml documentation" href="index.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        

        
          <a href="index.html" class="icon icon-home"> hep_ml
        

        
        </a>

        
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

        
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
          
          
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="">Gradient boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="#module-hep_ml.losses">Losses (loss functions for gradient boosting)</a></li>
<li class="toctree-l1"><a class="reference internal" href="uboost.html">uBoost</a><ul>
<li class="toctree-l2"><a class="reference internal" href="uboost.html#examples">Examples:</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metric functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nnet.html">Neural networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nnet.html#examples">Examples:</a></li>
</ul>
</li>
</ul>

          
        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">hep_ml</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Gradient boosting</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/gb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document">
            
  <div class="section" id="module-hep_ml.gradientboosting">
<span id="gradient-boosting"></span><h1>Gradient boosting<a class="headerlink" href="#module-hep_ml.gradientboosting" title="Permalink to this headline">¶</a></h1>
<p>Gradient boosting is general-purpose algorithm proposed by Friedman [1].
It is one of the most efficient machine learning algorithms used for classification, regression and ranking.</p>
<p>The key idea of algorithm is iterative minimization of target <strong>loss</strong> function
by training each time one more estimator to the sequence. In this implementation decision trees are taken as such estimators.</p>
<p>Important difference in loss functions available in <strong>hep_ml</strong>.
There are for instance, loss functions to fight with correlation or loss functions for ranking.
See <cite>hep_ml.losses</cite> for details.</p>
<p>see also: XGBoost library, sklearn.ensemble.GradientBoostingClassifier</p>
<table class="docutils footnote" frame="void" id="id1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>J.H. Friedman &#8216;Greedy function approximation: A gradient boosting machine.&#8217;, 2001.</td></tr>
</tbody>
</table>
<dl class="class">
<dt id="hep_ml.gradientboosting.GradientBoostingClassifier">
<em class="property">class </em><code class="descclassname">hep_ml.gradientboosting.</code><code class="descname">GradientBoostingClassifier</code><span class="sig-paren">(</span><em>loss=None</em>, <em>n_estimators=10</em>, <em>learning_rate=0.1</em>, <em>subsample=1.0</em>, <em>min_samples_split=2</em>, <em>min_samples_leaf=1</em>, <em>max_features=None</em>, <em>max_leaf_nodes=None</em>, <em>max_depth=3</em>, <em>criterion='mse'</em>, <em>splitter='best'</em>, <em>update_tree=True</em>, <em>train_features=None</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#GradientBoostingClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.gradientboosting.GradientBoostingClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">sklearn.base.BaseEstimator</span></code>, <code class="xref py py-class docutils literal"><span class="pre">sklearn.base.ClassifierMixin</span></code></p>
<p>This version of gradient boosting supports only two-class classification and only special losses
derived from AbstractLossFunction.</p>
<p><cite>max_depth</cite>, <cite>max_leaf_nodes</cite>, <cite>min_samples_leaf</cite>, <cite>min_samples_split</cite>, <cite>max_features</cite> are parameters
of regression tree, which is used as base estimator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>loss</strong> &#8211; any descendant of AbstractLossFunction, those are very various:
LogLossFunction, AdaLossFunction, KnnLossFunction, FlatnessLossFunction, RankBoostLossFunction.
See hep_ml.losses for details.</li>
<li><strong>n_estimators</strong> &#8211; int, number of trained trees.</li>
<li><strong>subsample</strong> &#8211; float, fraction of data to use on each stage</li>
<li><strong>learning_rate</strong> &#8211; size of step.</li>
<li><strong>update_tree</strong> &#8211; bool, True by default. If False, &#8216;improvement&#8217; step after fitting tree will be skipped.</li>
<li><strong>train_features</strong> &#8211; features used by tree.
Note that also there may be variables used by loss function, but not used in tree.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="hep_ml.gradientboosting.GradientBoostingClassifier.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#GradientBoostingClassifier.decision_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.gradientboosting.GradientBoostingClassifier.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Raw output, sum of trees&#8217; predictions
:param X: data
:return: numpy.array of shape [n_samples]</p>
</dd></dl>

<dl class="attribute">
<dt id="hep_ml.gradientboosting.GradientBoostingClassifier.feature_importances_">
<code class="descname">feature_importances_</code><a class="headerlink" href="#hep_ml.gradientboosting.GradientBoostingClassifier.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns feature importances for all features used in training.
The order corresponds to the order in <cite>self.train_features</cite>
:return: numpy.array of shape [n_train_features]</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.gradientboosting.GradientBoostingClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#GradientBoostingClassifier.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.gradientboosting.GradientBoostingClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Only two-class binary classification is supported with labels 0 and 1.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> &#8211; dataset of shape [n_samples, n_features]</li>
<li><strong>y</strong> &#8211; labels, array-like of shape [n_samples]</li>
<li><strong>sample_weight</strong> &#8211; array-like of shape [n_samples] or None</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">self</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hep_ml.gradientboosting.GradientBoostingClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#GradientBoostingClassifier.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.gradientboosting.GradientBoostingClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicted classes for each event
:param X: pandas.DataFrame with all train_features
:return: numpy.array of shape [n_samples] with predicted classes.</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.gradientboosting.GradientBoostingClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#GradientBoostingClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.gradientboosting.GradientBoostingClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicted probabilities for each event
:param X: pandas.DataFrame with all train_features
:return: numpy.array of shape [n_samples, n_classes]</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.gradientboosting.GradientBoostingClassifier.staged_decision_function">
<code class="descname">staged_decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#GradientBoostingClassifier.staged_decision_function"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.gradientboosting.GradientBoostingClassifier.staged_decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Raw output, sum of trees&#8217; predictions after each iteration.
:param X: data
:return: sequence of numpy.array of shape [n_samples]</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.gradientboosting.GradientBoostingClassifier.staged_predict_proba">
<code class="descname">staged_predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/gradientboosting.html#GradientBoostingClassifier.staged_predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.gradientboosting.GradientBoostingClassifier.staged_predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predicted probabilities for each event
:param X: data
:return: sequence of numpy.array of shape [n_samples, n_classes]</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-hep_ml.losses">
<span id="losses-loss-functions-for-gradient-boosting"></span><h1>Losses (loss functions for gradient boosting)<a class="headerlink" href="#module-hep_ml.losses" title="Permalink to this headline">¶</a></h1>
<p><strong>hep_ml.losses</strong> contains different loss functions to use in gradient boosting.</p>
<p>Apart from standard classification losses, <strong>hep_ml</strong> contains losses for uniform classification
(see BinFlatnessLoss, KnnFlatnessLoss) and for ranking (see RankBoostLossFunction)</p>
<dl class="class">
<dt id="hep_ml.losses.AbstractFlatnessLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">AbstractFlatnessLossFunction</code><span class="sig-paren">(</span><em>uniform_features</em>, <em>uniform_label=1</em>, <em>power=2.0</em>, <em>ada_coefficient=1.0</em>, <em>allow_wrong_signs=True</em>, <em>use_median=False</em>, <em>keep_debug_info=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractFlatnessLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractFlatnessLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.AbstractLossFunction" title="hep_ml.losses.AbstractLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.AbstractLossFunction</span></code></a></p>
<p>This loss function contains separately penalty for non-flatness and ada_coefficient.
The penalty for non-flatness is using bins.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
</tbody>
</table>
<dl class="method">
<dt id="hep_ml.losses.AbstractFlatnessLossFunction.compute_groups_indices">
<code class="descname">compute_groups_indices</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>label</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractFlatnessLossFunction.compute_groups_indices"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractFlatnessLossFunction.compute_groups_indices" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractFlatnessLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractFlatnessLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractFlatnessLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractFlatnessLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractFlatnessLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractFlatnessLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.AbstractLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">AbstractLossFunction</code><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>This is base class for loss functions used in <cite>hep_ml</cite>.
Main differences compared to <cite>scikit-learn</cite> loss functions:</p>
<ol class="arabic simple">
<li>losses are stateful, and may require fitting of training data before usage.</li>
<li>thus, when computing gradient, hessian, one shall provide predictions of all events.</li>
<li>losses are object that shall be passed as estimators to gradient boosting (see examples).</li>
<li>only two-class case is supported, and different classes may have different role and meaning.</li>
</ol>
<dl class="method">
<dt id="hep_ml.losses.AbstractLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is optional, it is called before all the others.</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>The y_pred should contain all the events passed to <cite>fit</cite> method,
moreover, the order should be the same</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractLossFunction.prepare_new_leaves_values">
<code class="descname">prepare_new_leaves_values</code><span class="sig-paren">(</span><em>terminal_regions</em>, <em>leaf_values</em>, <em>X</em>, <em>y</em>, <em>y_pred</em>, <em>sample_weight</em>, <em>update_mask</em>, <em>residual</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractLossFunction.prepare_new_leaves_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractLossFunction.prepare_new_leaves_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Method for pruning. Loss function can prepare better values for leaves</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>terminal_regions</strong> &#8211; indices of terminal regions of each event.</li>
<li><strong>leaf_values</strong> &#8211; numpy.array, current mapping of leaf indices to prediction values.</li>
<li><strong>X</strong> &#8211; data (same as passed in fit, ignored usually)</li>
<li><strong>y</strong> &#8211; labels (same as passed in fit)</li>
<li><strong>y_pred</strong> &#8211; predictions before adding new tree.</li>
<li><strong>sample_weight</strong> &#8211; weights od samples (same as passed in fit)</li>
<li><strong>update_mask</strong> &#8211; which events to use during update?</li>
<li><strong>residual</strong> &#8211; computed value of negative gradient (before adding tree)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">numpy.array with new prediction values for all leaves.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractLossFunction.prepare_tree_params">
<code class="descname">prepare_tree_params</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractLossFunction.prepare_tree_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractLossFunction.prepare_tree_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepares parameters for regression tree that minimizes MSE</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>y_pred</strong> &#8211; contains predictions for all the events passed to <cite>fit</cite> method,
moreover, the order should be the same</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">tuple (residual, sample_weight) with target and weight to be used in decision tree</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.AbstractMatrixLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">AbstractMatrixLossFunction</code><span class="sig-paren">(</span><em>uniform_features</em>, <em>regularization=5.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractMatrixLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractMatrixLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.HessianLossFunction" title="hep_ml.losses.HessianLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.HessianLossFunction</span></code></a></p>
<p>KnnLossFunction is a base class to be inherited by other loss functions,
which choose the particular A matrix and w vector. The formula of loss is:
loss = sum_i w_i * exp(- sum_j a_ij y_j score_j)</p>
<dl class="method">
<dt id="hep_ml.losses.AbstractMatrixLossFunction.compute_parameters">
<code class="descname">compute_parameters</code><span class="sig-paren">(</span><em>trainX</em>, <em>trainY</em>, <em>trainW</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractMatrixLossFunction.compute_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractMatrixLossFunction.compute_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>This method should be overloaded in descendant, and should return A, w (matrix and vector)</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractMatrixLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractMatrixLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractMatrixLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is used to compute A matrix and w based on train dataset</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractMatrixLossFunction.hessian">
<code class="descname">hessian</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractMatrixLossFunction.hessian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractMatrixLossFunction.hessian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractMatrixLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractMatrixLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractMatrixLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Computing negative gradient</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractMatrixLossFunction.prepare_new_leaves_values">
<code class="descname">prepare_new_leaves_values</code><span class="sig-paren">(</span><em>terminal_regions</em>, <em>leaf_values</em>, <em>X</em>, <em>y</em>, <em>y_pred</em>, <em>sample_weight</em>, <em>update_mask</em>, <em>residual</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractMatrixLossFunction.prepare_new_leaves_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractMatrixLossFunction.prepare_new_leaves_values" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.AdaLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">AdaLossFunction</code><span class="sig-paren">(</span><em>regularization=5.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AdaLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.HessianLossFunction" title="hep_ml.losses.HessianLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.HessianLossFunction</span></code></a></p>
<p>AdaLossFunction is the same as Exponential Loss Function (aka exploss)</p>
<dl class="method">
<dt id="hep_ml.losses.AdaLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AdaLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AdaLossFunction.hessian">
<code class="descname">hessian</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction.hessian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AdaLossFunction.hessian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AdaLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AdaLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.BinFlatnessLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">BinFlatnessLossFunction</code><span class="sig-paren">(</span><em>uniform_features</em>, <em>n_bins=10</em>, <em>uniform_label=1</em>, <em>power=2.0</em>, <em>ada_coefficient=1.0</em>, <em>allow_wrong_signs=True</em>, <em>use_median=False</em>, <em>keep_debug_info=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#BinFlatnessLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.BinFlatnessLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.AbstractFlatnessLossFunction" title="hep_ml.losses.AbstractFlatnessLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.AbstractFlatnessLossFunction</span></code></a></p>
<dl class="method">
<dt id="hep_ml.losses.BinFlatnessLossFunction.compute_groups_indices">
<code class="descname">compute_groups_indices</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>label</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#BinFlatnessLossFunction.compute_groups_indices"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.BinFlatnessLossFunction.compute_groups_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list, each element is events&#8217; indices in some group.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.BinomialDevianceLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">BinomialDevianceLossFunction</code><span class="sig-paren">(</span><em>regularization=5.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#BinomialDevianceLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.BinomialDevianceLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.HessianLossFunction" title="hep_ml.losses.HessianLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.HessianLossFunction</span></code></a></p>
<p>Binomial deviance, aka Logistic loss function (logloss).</p>
<dl class="method">
<dt id="hep_ml.losses.BinomialDevianceLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#BinomialDevianceLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.BinomialDevianceLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.BinomialDevianceLossFunction.hessian">
<code class="descname">hessian</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#BinomialDevianceLossFunction.hessian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.BinomialDevianceLossFunction.hessian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.BinomialDevianceLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#BinomialDevianceLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.BinomialDevianceLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.CompositeLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">CompositeLossFunction</code><span class="sig-paren">(</span><em>regularization=5.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#CompositeLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.CompositeLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.HessianLossFunction" title="hep_ml.losses.HessianLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.HessianLossFunction</span></code></a></p>
<p>Composite loss function is defined as exploss for backgorund events and logloss for signal with proper constants.</p>
<p>Such kind of loss functions is very useful to optimize AMS or other function.</p>
<dl class="method">
<dt id="hep_ml.losses.CompositeLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#CompositeLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.CompositeLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.CompositeLossFunction.hessian">
<code class="descname">hessian</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#CompositeLossFunction.hessian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.CompositeLossFunction.hessian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.CompositeLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#CompositeLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.CompositeLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.HessianLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">HessianLossFunction</code><span class="sig-paren">(</span><em>regularization=5.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#HessianLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.HessianLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.AbstractLossFunction" title="hep_ml.losses.AbstractLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.AbstractLossFunction</span></code></a></p>
<p>Loss function with diagonal hessian, provides uses Newton-Raphson step to update trees.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>regularization</strong> &#8211; float, penalty for leaves with few events,
corresponds roughly to the number of added events of both classes to each leaf.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="hep_ml.losses.HessianLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#HessianLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.HessianLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.HessianLossFunction.hessian">
<code class="descname">hessian</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#HessianLossFunction.hessian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.HessianLossFunction.hessian" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns diagonal of hessian matrix.
:param y_pred: numpy.array of shape [n_samples] with events passed in the same order as in <cite>fit</cite>.
:return: numpy.array of shape [n_sampels] with second derivatives with respect to each prediction.</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.HessianLossFunction.prepare_new_leaves_values">
<code class="descname">prepare_new_leaves_values</code><span class="sig-paren">(</span><em>terminal_regions</em>, <em>leaf_values</em>, <em>X</em>, <em>y</em>, <em>y_pred</em>, <em>sample_weight</em>, <em>update_mask</em>, <em>residual</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#HessianLossFunction.prepare_new_leaves_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.HessianLossFunction.prepare_new_leaves_values" title="Permalink to this definition">¶</a></dt>
<dd><p>This expression comes from optimization of second-order approximation of loss function.</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.HessianLossFunction.prepare_tree_params">
<code class="descname">prepare_tree_params</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#HessianLossFunction.prepare_tree_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.HessianLossFunction.prepare_tree_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.KnnFlatnessLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">KnnFlatnessLossFunction</code><span class="sig-paren">(</span><em>uniform_features</em>, <em>n_neighbours=100</em>, <em>uniform_label=1</em>, <em>power=2.0</em>, <em>ada_coefficient=1.0</em>, <em>max_groups_on_iteration=3000</em>, <em>allow_wrong_signs=True</em>, <em>use_median=False</em>, <em>keep_debug_info=False</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#KnnFlatnessLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.KnnFlatnessLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.AbstractFlatnessLossFunction" title="hep_ml.losses.AbstractFlatnessLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.AbstractFlatnessLossFunction</span></code></a></p>
<dl class="method">
<dt id="hep_ml.losses.KnnFlatnessLossFunction.compute_groups_indices">
<code class="descname">compute_groups_indices</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>label</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#KnnFlatnessLossFunction.compute_groups_indices"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.KnnFlatnessLossFunction.compute_groups_indices" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.RankBoostLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">RankBoostLossFunction</code><span class="sig-paren">(</span><em>request_column</em>, <em>messup_penalty='square'</em>, <em>regularization=0.1</em>, <em>update_iterations=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.RankBoostLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.HessianLossFunction" title="hep_ml.losses.HessianLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.HessianLossFunction</span></code></a></p>
<dl class="method">
<dt id="hep_ml.losses.RankBoostLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.RankBoostLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.RankBoostLossFunction.hessian">
<code class="descname">hessian</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.hessian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.RankBoostLossFunction.hessian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.RankBoostLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.RankBoostLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.RankBoostLossFunction.prepare_new_leaves_values">
<code class="descname">prepare_new_leaves_values</code><span class="sig-paren">(</span><em>terminal_regions</em>, <em>leaf_values</em>, <em>X</em>, <em>y</em>, <em>y_pred</em>, <em>sample_weight</em>, <em>update_mask</em>, <em>residual</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.prepare_new_leaves_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.RankBoostLossFunction.prepare_new_leaves_values" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.ReweightLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">ReweightLossFunction</code><span class="sig-paren">(</span><em>regularization=5.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#ReweightLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.ReweightLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.AbstractLossFunction" title="hep_ml.losses.AbstractLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.AbstractLossFunction</span></code></a></p>
<dl class="docutils">
<dt>Loss function used to reweight events. Conventions:</dt>
<dd>y=0 - target distribution, y=1 - original distribution.</dd>
<dt>Weights after look like:</dt>
<dd>w = w_0 for target distribution
w = w_0 * exp(pred) for events from original distribution
(so pred for target distribution is ignored)</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>regularization</strong> &#8211; roughly, it&#8217;s number of events added in each leaf to prevent overfitting.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="hep_ml.losses.ReweightLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#ReweightLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.ReweightLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.ReweightLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#ReweightLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.ReweightLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.ReweightLossFunction.prepare_new_leaves_values">
<code class="descname">prepare_new_leaves_values</code><span class="sig-paren">(</span><em>terminal_regions</em>, <em>leaf_values</em>, <em>X</em>, <em>y</em>, <em>y_pred</em>, <em>sample_weight</em>, <em>update_mask</em>, <em>residual</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#ReweightLossFunction.prepare_new_leaves_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.ReweightLossFunction.prepare_new_leaves_values" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.ReweightLossFunction.prepare_tree_params">
<code class="descname">prepare_tree_params</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#ReweightLossFunction.prepare_tree_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.ReweightLossFunction.prepare_tree_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.ReweightNegativeLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">ReweightNegativeLossFunction</code><span class="sig-paren">(</span><em>regularization=5.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#ReweightNegativeLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.ReweightNegativeLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.AbstractLossFunction" title="hep_ml.losses.AbstractLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.AbstractLossFunction</span></code></a></p>
<p>Attempt to support negative weights.</p>
<dl class="method">
<dt id="hep_ml.losses.ReweightNegativeLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#ReweightNegativeLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.ReweightNegativeLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.ReweightNegativeLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#ReweightNegativeLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.ReweightNegativeLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.ReweightNegativeLossFunction.prepare_new_leaves_values">
<code class="descname">prepare_new_leaves_values</code><span class="sig-paren">(</span><em>terminal_regions</em>, <em>leaf_values</em>, <em>X</em>, <em>y</em>, <em>y_pred</em>, <em>sample_weight</em>, <em>update_mask</em>, <em>residual</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#ReweightNegativeLossFunction.prepare_new_leaves_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.ReweightNegativeLossFunction.prepare_new_leaves_values" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.ReweightNegativeLossFunction.prepare_tree_params">
<code class="descname">prepare_tree_params</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#ReweightNegativeLossFunction.prepare_tree_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.ReweightNegativeLossFunction.prepare_tree_params" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.SimpleKnnLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">SimpleKnnLossFunction</code><span class="sig-paren">(</span><em>uniform_features</em>, <em>knn=10</em>, <em>uniform_label=1</em>, <em>distinguish_classes=True</em>, <em>row_norm=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#SimpleKnnLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.SimpleKnnLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#hep_ml.losses.AbstractMatrixLossFunction" title="hep_ml.losses.AbstractMatrixLossFunction"><code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.AbstractMatrixLossFunction</span></code></a></p>
<p>A matrix is square, each row corresponds to a single event in train dataset, in each row we put ones
to the closest neighbours of that event if this event from class along which we want to have uniform prediction.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>uniform_features</strong> (<em>list[str]</em>) &#8211; the features, along which uniformity is desired</li>
<li><strong>knn</strong> (<em>int</em>) &#8211; the number of nonzero elements in the row, corresponding to event in &#8216;uniform class&#8217;</li>
<li><strong>uniform_label</strong> (<em>int|list[int]</em>) &#8211; the label (labels) of &#8216;uniform classes&#8217;</li>
<li><strong>distinguish_classes</strong> (<em>bool</em>) &#8211; if True, 1&#8217;s will be placed only for events of same class.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="hep_ml.losses.SimpleKnnLossFunction.compute_parameters">
<code class="descname">compute_parameters</code><span class="sig-paren">(</span><em>trainX</em>, <em>trainY</em>, <em>trainW</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#SimpleKnnLossFunction.compute_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.SimpleKnnLossFunction.compute_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="function">
<dt id="hep_ml.losses.exp_margin">
<code class="descclassname">hep_ml.losses.</code><code class="descname">exp_margin</code><span class="sig-paren">(</span><em>margin</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#exp_margin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.exp_margin" title="Permalink to this definition">¶</a></dt>
<dd><p>margin = - y_signed * y_pred</p>
</dd></dl>

</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="uboost.html" class="btn btn-neutral float-right" title="uBoost" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="hep_ml documentation" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Yandex; Alex Rogozhnikov and contributors.
    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>