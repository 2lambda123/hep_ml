

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Losses for Gradient Boosting &mdash; hep_ml 0.2 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="hep_ml 0.2 documentation" href="index.html"/>
        <link rel="next" title="uBoost" href="uboost.html"/>
        <link rel="prev" title="Gradient boosting" href="gb.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        

        
          <a href="index.html" class="icon icon-home"> hep_ml
        

        
        </a>

        
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

        
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
          
          
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gb.html">Gradient boosting</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="">Losses for Gradient Boosting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="uboost.html">uBoost</a><ul>
<li class="toctree-l2"><a class="reference internal" href="uboost.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="metrics.html">Metric functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="metrics.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="nnet.html">Neural networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="nnet.html#examples">Examples</a></li>
</ul>
</li>
</ul>

          
        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">hep_ml</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>Losses for Gradient Boosting</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/losses.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document">
            
  <div class="section" id="module-hep_ml.losses">
<span id="losses-for-gradient-boosting"></span><h1>Losses for Gradient Boosting<a class="headerlink" href="#module-hep_ml.losses" title="Permalink to this headline">¶</a></h1>
<p><strong>hep_ml.losses</strong> contains different loss functions to use in gradient boosting.</p>
<p>Apart from standard classification losses, <strong>hep_ml</strong> contains losses for uniform classification
(see BinFlatnessLoss, KnnFlatnessLoss, KnnAdaLossFunction) and for ranking (see RankBoostLossFunction)</p>
<p><strong>Interface</strong></p>
<p>Loss functions inside <strong>hep_ml</strong> are stateful estimators and require initial fitting,
which is done automatically inside gradient boosting.</p>
<p>All loss function should be derived from AbstractLossFunction and implement this interface.</p>
<div class="section" id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<p>Training gradient boosting, optimizing LogLoss and using all features</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">hep_ml.gradientboosting</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span><span class="p">,</span> <span class="n">LogLossFunction</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">LogLossFunction</span><span class="p">(),</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
</pre></div>
</div>
<p>Using composite loss function and subsampling:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">CompositeLossFunction</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<p>To get uniform predictions in mass in background (note that mass should not present in features):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">BinFlatnessLossFunction</span><span class="p">(</span><span class="n">uniform_features</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;mass&#39;</span><span class="p">],</span> <span class="n">uniform_label</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                               <span class="n">ada_coefficient</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">train_features</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;pt&#39;</span><span class="p">,</span> <span class="s">&#39;flight_time&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>To get uniform predictions in both signal and background:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">BinFlatnessLossFunction</span><span class="p">(</span><span class="n">uniform_features</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;mass&#39;</span><span class="p">],</span> <span class="n">uniform_label</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>                               <span class="n">ada_coefficient</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">train_features</span><span class="o">=</span><span class="p">[</span><span class="s">&#39;pt&#39;</span><span class="p">,</span> <span class="s">&#39;flight_time&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<dl class="class">
<dt id="hep_ml.losses.AbstractLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">AbstractLossFunction</code><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>This is base class for loss functions used in <cite>hep_ml</cite>.
Main differences compared to <cite>scikit-learn</cite> loss functions:</p>
<ol class="arabic simple">
<li>losses are stateful, and may require fitting of training data before usage.</li>
<li>thus, when computing gradient, hessian, one shall provide predictions of all events.</li>
<li>losses are object that shall be passed as estimators to gradient boosting (see examples).</li>
<li>only two-class case is supported, and different classes may have different role and meaning.</li>
</ol>
<dl class="method">
<dt id="hep_ml.losses.AbstractLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is optional, it is called before all the others.</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>The y_pred should contain all the events passed to <cite>fit</cite> method,
moreover, the order should be the same</p>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractLossFunction.prepare_new_leaves_values">
<code class="descname">prepare_new_leaves_values</code><span class="sig-paren">(</span><em>terminal_regions</em>, <em>leaf_values</em>, <em>X</em>, <em>y</em>, <em>y_pred</em>, <em>sample_weight</em>, <em>update_mask</em>, <em>residual</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractLossFunction.prepare_new_leaves_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractLossFunction.prepare_new_leaves_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Method for pruning. Loss function can prepare better values for leaves</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>terminal_regions</strong> &#8211; indices of terminal regions of each event.</li>
<li><strong>leaf_values</strong> &#8211; numpy.array, current mapping of leaf indices to prediction values.</li>
<li><strong>X</strong> &#8211; data (same as passed in fit, ignored usually)</li>
<li><strong>y</strong> &#8211; labels (same as passed in fit)</li>
<li><strong>y_pred</strong> &#8211; predictions before adding new tree.</li>
<li><strong>sample_weight</strong> &#8211; weights od samples (same as passed in fit)</li>
<li><strong>update_mask</strong> &#8211; which events to use during update?</li>
<li><strong>residual</strong> &#8211; computed value of negative gradient (before adding tree)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">numpy.array with new prediction values for all leaves.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AbstractLossFunction.prepare_tree_params">
<code class="descname">prepare_tree_params</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AbstractLossFunction.prepare_tree_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AbstractLossFunction.prepare_tree_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepares parameters for regression tree that minimizes MSE</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>y_pred</strong> &#8211; contains predictions for all the events passed to <cite>fit</cite> method,
moreover, the order should be the same</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">tuple (residual, sample_weight) with target and weight to be used in decision tree</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.LogLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">LogLossFunction</code><span class="sig-paren">(</span><em>regularization=5.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#LogLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.LogLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.HessianLossFunction</span></code></p>
<p>Logistic loss function (logloss), aka binomial deviance, aka cross-entropy,
aka log-likelihood loss.</p>
<dl class="method">
<dt id="hep_ml.losses.LogLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#LogLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.LogLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.LogLossFunction.hessian">
<code class="descname">hessian</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#LogLossFunction.hessian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.LogLossFunction.hessian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.LogLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#LogLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.LogLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.AdaLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">AdaLossFunction</code><span class="sig-paren">(</span><em>regularization=5.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AdaLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.HessianLossFunction</span></code></p>
<p>AdaLossFunction is the same as Exponential Loss Function (aka exploss)</p>
<dl class="method">
<dt id="hep_ml.losses.AdaLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AdaLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AdaLossFunction.hessian">
<code class="descname">hessian</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction.hessian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AdaLossFunction.hessian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.AdaLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#AdaLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.AdaLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.CompositeLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">CompositeLossFunction</code><span class="sig-paren">(</span><em>regularization=5.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#CompositeLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.CompositeLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.HessianLossFunction</span></code></p>
<p>Composite loss function is defined as exploss for backgorund events and logloss for signal with proper constants.</p>
<p>Such kind of loss functions is very useful to optimize AMS or in situations where very clean signal is expected.</p>
<dl class="method">
<dt id="hep_ml.losses.CompositeLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#CompositeLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.CompositeLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.CompositeLossFunction.hessian">
<code class="descname">hessian</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#CompositeLossFunction.hessian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.CompositeLossFunction.hessian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.CompositeLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#CompositeLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.CompositeLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.BinFlatnessLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">BinFlatnessLossFunction</code><span class="sig-paren">(</span><em>uniform_features</em>, <em>n_bins=10</em>, <em>uniform_label=1</em>, <em>power=2.0</em>, <em>ada_coefficient=1.0</em>, <em>allow_wrong_signs=True</em>, <em>use_median=False</em>, <em>keep_debug_info=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#BinFlatnessLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.BinFlatnessLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.AbstractFlatnessLossFunction</span></code></p>
<dl class="method">
<dt id="hep_ml.losses.BinFlatnessLossFunction.compute_groups_indices">
<code class="descname">compute_groups_indices</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>label</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#BinFlatnessLossFunction.compute_groups_indices"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.BinFlatnessLossFunction.compute_groups_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list, each element is events&#8217; indices in some group.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.KnnFlatnessLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">KnnFlatnessLossFunction</code><span class="sig-paren">(</span><em>uniform_features</em>, <em>n_neighbours=100</em>, <em>uniform_label=1</em>, <em>power=2.0</em>, <em>ada_coefficient=1.0</em>, <em>max_groups_on_iteration=3000</em>, <em>allow_wrong_signs=True</em>, <em>use_median=False</em>, <em>keep_debug_info=False</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#KnnFlatnessLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.KnnFlatnessLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.AbstractFlatnessLossFunction</span></code></p>
<dl class="method">
<dt id="hep_ml.losses.KnnFlatnessLossFunction.compute_groups_indices">
<code class="descname">compute_groups_indices</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>label</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#KnnFlatnessLossFunction.compute_groups_indices"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.KnnFlatnessLossFunction.compute_groups_indices" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.KnnAdaLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">KnnAdaLossFunction</code><span class="sig-paren">(</span><em>uniform_features</em>, <em>knn=10</em>, <em>uniform_label=1</em>, <em>distinguish_classes=True</em>, <em>row_norm=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#KnnAdaLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.KnnAdaLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.AbstractMatrixLossFunction</span></code></p>
<p>The formula of loss is:
<span class="math">\(loss = \sum_i w_i * exp(- \sum_j a_{ij} y_j score_j)\)</span></p>
<p><cite>A</cite> matrix is square, each row corresponds to a single event in train dataset, in each row we put ones
to the closest neighbours of that event if this event from class along which we want to have uniform prediction.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>uniform_features</strong> (<em>list[str]</em>) &#8211; the features, along which uniformity is desired</li>
<li><strong>knn</strong> (<em>int</em>) &#8211; the number of nonzero elements in the row, corresponding to event in &#8216;uniform class&#8217;</li>
<li><strong>uniform_label</strong> (<em>int|list[int]</em>) &#8211; the label (labels) of &#8216;uniform classes&#8217;</li>
<li><strong>distinguish_classes</strong> (<em>bool</em>) &#8211; if True, 1&#8217;s will be placed only for events of same class.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="hep_ml.losses.KnnAdaLossFunction.compute_parameters">
<code class="descname">compute_parameters</code><span class="sig-paren">(</span><em>trainX</em>, <em>trainY</em>, <em>trainW</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#KnnAdaLossFunction.compute_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.KnnAdaLossFunction.compute_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="class">
<dt id="hep_ml.losses.RankBoostLossFunction">
<em class="property">class </em><code class="descclassname">hep_ml.losses.</code><code class="descname">RankBoostLossFunction</code><span class="sig-paren">(</span><em>request_column</em>, <em>messup_penalty='square'</em>, <em>update_iterations=1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.RankBoostLossFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">hep_ml.losses.HessianLossFunction</span></code></p>
<p>RankBoostLossFunction is target of optimization in RankBoost algorithm,
which was developed for ranking and introduces penalties for wrong order of predictions.</p>
<p>However, this implementation goes further and there is selection of optimal leaf values based
on iterative procedure.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request_column</strong> (<em>str</em>) &#8211; name of column with search query ids.</li>
<li><strong>messup_penalty</strong> (<em>str</em>) &#8211; &#8216;linear&#8217; or &#8216;square&#8217;, dependency of penalty on difference in target values.</li>
<li><strong>update_iterations</strong> (<em>int</em>) &#8211; number of minimization steps to provide optimal values.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="hep_ml.losses.RankBoostLossFunction.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>sample_weight</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.RankBoostLossFunction.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.RankBoostLossFunction.hessian">
<code class="descname">hessian</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.hessian"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.RankBoostLossFunction.hessian" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.RankBoostLossFunction.negative_gradient">
<code class="descname">negative_gradient</code><span class="sig-paren">(</span><em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.negative_gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.RankBoostLossFunction.negative_gradient" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="hep_ml.losses.RankBoostLossFunction.prepare_new_leaves_values">
<code class="descname">prepare_new_leaves_values</code><span class="sig-paren">(</span><em>terminal_regions</em>, <em>leaf_values</em>, <em>X</em>, <em>y</em>, <em>y_pred</em>, <em>sample_weight</em>, <em>update_mask</em>, <em>residual</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/hep_ml/losses.html#RankBoostLossFunction.prepare_new_leaves_values"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#hep_ml.losses.RankBoostLossFunction.prepare_new_leaves_values" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="uboost.html" class="btn btn-neutral float-right" title="uBoost" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="gb.html" class="btn btn-neutral" title="Gradient boosting" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Yandex; Alex Rogozhnikov and contributors.
    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>